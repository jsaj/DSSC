{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSSC_estudo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsaj/dssc/blob/master/DSSC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vs1UWSyphKD"
      },
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from warnings import filterwarnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# base classifiers\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import f1_score, auc, roc_auc_score\n",
        "from scipy import *\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71WhOIGxRRJP"
      },
      "source": [
        "class DSSC():\n",
        "  \"\"\"Dynamic Selection Supervised Cross-project defect prediction (DSSC).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "  References\n",
        "    ----------\n",
        "\n",
        "    R. M. Cruz, L. G. Hafemann, R. Sabourin, and G. D. Cavalcanti.\n",
        "    \"Deslib:  A dynamicensemble selection library in python.\"\n",
        "    Journal of Machine Learning Research, vol. 21, no. 8, pp. 1–5, 2020.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, url_dataset, with_PF=True):\n",
        "\n",
        "    dataset_name = url_dataset.split('/')\n",
        "    self.dataset_name = dataset_name[len(dataset_name)-1]\n",
        "    self.url_dataset = url_dataset + '/*'\n",
        "    # self.pf_has_been_called = False\n",
        "\n",
        "    dataset_total = []\n",
        "\n",
        "    for project in glob(self.url_dataset):\n",
        "      project_name = project.split('/')\n",
        "      project_name = project_name[len(project_name)-1].split('.csv')[0]\n",
        "      ds = pd.read_csv(project)\n",
        "\n",
        "      if 'name' in list(ds.columns):\n",
        "        ds.pop('name')\n",
        "        ds['name'] = project_name\n",
        "      else:\n",
        "        ds['name'] = project_name\n",
        "      dataset_total.append(ds)\n",
        "    \n",
        "    dataset_total = pd.concat(dataset_total).reset_index(drop=True)\n",
        "\n",
        "    #project filtering stage\n",
        "    if with_PF == True:\n",
        "      dataset_total = self._project_filtering(dataset_total)\n",
        "\n",
        "    self.dataset_total = dataset_total\n",
        "    self.train, self.test = [], []\n",
        "    self.percent_bugs = 0\n",
        "  \n",
        "  def _project_filtering(self, dataset_total):\n",
        "\n",
        "    \"\"\"Filter of projects for prediction.\n",
        "\n",
        "    Each project and its versions are checked to see if they have a minimum\n",
        "    number of 5 instances of each label (defect and non-defect).\n",
        " \n",
        "    Parameters\n",
        "    ----------\n",
        "    with_PF : Boolean (Default = False)\n",
        "        Determines if the filter is applied to check if project have a minimum\n",
        "        number of instances.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    dataset_total : DataFrame\n",
        "        DataFrame containing all projects with minimum number of instances.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    S. Herbold,  A. Trautsch,  and J. Grabowski. \"A comparative study to\n",
        "    benchmark cross-project  defect  prediction  approaches\". IEEE  Transactions\n",
        "    on  Software  Engineering, vol. 44, no. 9, pp. 811–833, 2017.\n",
        "\n",
        "    \"\"\"\n",
        "    return dataset_total\n",
        "\n",
        "  def _target_definition(self, target_project, dataset_total):\n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    \n",
        "    target_project : string\n",
        "        String with name of target project.\n",
        "\n",
        "    dataset_total : DataFrame\n",
        "        DataFrame containing all projects for prediction\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train : {DataFrame} of shape (n_samples, n_features)\n",
        "        The training input samples.\n",
        "\n",
        "    test : {DataFrame} of shape (n_samples, n_features)\n",
        "        The target project input samples.\n",
        "\n",
        "    \"\"\"\n",
        "    target_name = target_project.split('.csv')[0]\n",
        "\n",
        "    if '-' in target_name:\n",
        "      target_name = target_project.split('-')[0]\n",
        "\n",
        "    if '.' in target_name:\n",
        "      target_name = target_project.split('.')[0]\n",
        "\n",
        "    for character in target_name:\n",
        "      if character.isdigit():\n",
        "        target_name = list(re.findall(r'(\\w+?)(\\d+)', target_name)[0])\n",
        "        target_name = target_name[0]\n",
        "        break\n",
        "\n",
        "    test_data = dataset_total.loc[dataset_total['name'] == target_project]\n",
        "    test_data = test_data.select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "    bugs = list(np.unique(test_data[test_data.columns[0]]))[1:]\n",
        "    test_data[test_data.columns[0]] = test_data[test_data.columns[0]].replace(bugs, 1)\n",
        "    \n",
        "    train_data = dataset_total[~dataset_total['name'].str.contains(target_name)]\n",
        "    for project_name in list(np.unique(train_data['name'])):\n",
        "      ds = train_data.loc[train_data['name'] == project_name]\n",
        "      y = ds[ds.columns[0]]\n",
        "      bugs = list(np.unique(y))[1:]\n",
        "      y = y.replace(bugs, 1)\n",
        "      defective = round((np.count_nonzero(np.array(y) == 1) / len(y)) * 100, 2)\n",
        "      no_defective = round((np.count_nonzero(np.array(y) == 0) / len(y)) * 100, 2)\n",
        "\n",
        "      if defective < 5.0 or no_defective < 5.0 or len(ds) < 100:\n",
        "        train_data = train_data[train_data['name'] != project_name]\n",
        "    train_data = train_data.select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "    bugs = list(np.unique(train_data[train_data.columns[0]]))[1:]\n",
        "    train_data[train_data.columns[0]] = train_data[train_data.columns[0]].replace(bugs, 1)\n",
        "\n",
        "    def Diff(li1, li2):\n",
        "      return list(set(li1) - set(li2)) + list(set(li2) - set(li1))\n",
        "\n",
        "    diff_columns = Diff(list(train_data.columns), list(test_data.columns))\n",
        "    \n",
        "    if len(diff_columns) > 0:\n",
        "      for col in diff_columns:\n",
        "        if col in list(train_data.columns):\n",
        "          train_data.pop(col)\n",
        "        elif col in list(test_data.columns):\n",
        "          test_data.pop(col)\n",
        "\n",
        "   \n",
        "    self.test = test_data\n",
        "    self.train = train_data\n",
        "\n",
        "    y = self.test[self.test.columns[0]]\n",
        "    bugs = list(np.unique(y))[1:]\n",
        "    y = y.replace(bugs, 1)\n",
        "    self.percent_bugs = round((np.count_nonzero(np.array(y) == 1) / len(y)) * 100, 2)\n",
        "\n",
        "  def _calc_popt(self, defective, LOC, effort):\n",
        "    \n",
        "    effort_instances =  (effort * defective[LOC].sum())/100\n",
        "    index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    TargetList = defective[:index]\n",
        "    y_test = TargetList[TargetList.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1) \n",
        "    effort_percent = np.arange(0, 101, 1)\n",
        "    defective_list = []\n",
        "\n",
        "    for percent in range(0, 101):\n",
        "      effort_loc =  (percent * TargetList[LOC].sum())/100\n",
        "      index = TargetList[LOC].cumsum().searchsorted(effort_loc)\n",
        "      data = defective[:index]\n",
        "      \n",
        "      if len(data) != 0 and percent < 100:\n",
        "        bugs = np.count_nonzero(data[data.columns[0]] == 1)\n",
        "        percent_bugs = bugs / np.count_nonzero( y_test == 1)\n",
        "        defective_list.append(percent_bugs)\n",
        "      elif len(data) != 0 and percent == 100:\n",
        "        percent_bugs = 1.0\n",
        "        defective_list.append(percent_bugs)\n",
        "      else:\n",
        "        percent_bugs = 0.0\n",
        "        defective_list.append(percent_bugs)\n",
        "      \n",
        "    x = effort_percent\n",
        "    y = defective_list\n",
        "\n",
        "    y_a = np.arange(0.0, 1.01, 0.02)\n",
        "    x_b = np.arange(0, 101, 2)\n",
        "\n",
        "    x_a, y_b, h = [], [], []\n",
        "    for i in range(51):\n",
        "      x_a.append(0)\n",
        "      y_b.append(1)\n",
        "      h.append(100)\n",
        "\n",
        "    x1 = np.concatenate([x_a, x_b])\n",
        "    y1  = np.concatenate([y_a, y_b])\n",
        "\n",
        "    x2 = np.concatenate([x_b, h])\n",
        "    y2 = np.concatenate([x_a, y_a])\n",
        "\n",
        "    area_P_R = auc(x, y) - auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "    area_O_P = auc(x1, y1) - auc(x, y)\n",
        "    area_P_R = auc(x, y) - auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "    area_R_W = auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01)) - auc(x2, y2)\n",
        "\n",
        "    popt = 1 - (area_O_P/ (area_O_P + area_P_R + area_R_W))\n",
        "    if popt > 1:\n",
        "      popt = 1.0\n",
        "    return popt\n",
        "  \n",
        "  def _calf_IFA(self, defective, LOC, model, scaler, effort):\n",
        "    X_test = defective.drop(defective.columns[0], axis=1)\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    defective['predict'] = y_pred\n",
        "    effort_instances =  (effort * defective[LOC].sum())/100\n",
        "    index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    TargetList = defective[:index]\n",
        "    y_test = TargetList[TargetList.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1) \n",
        "    \n",
        "    IFA = 0\n",
        "    for pred, true in zip(y_pred, y_test):\n",
        "      if true == 1 and pred == 1:\n",
        "        break\n",
        "      elif true == 0 and pred == 1:\n",
        "        IFA +=1\n",
        "\n",
        "    return IFA\n",
        "  \n",
        "  def _calc_PIIL_CEL(self, defective, LOC, model, scaler, effort):\n",
        "    \n",
        "    if 'predict' in list(defective.columns):\n",
        "      defective = defective.drop('predict', axis=1)\n",
        "    X_test = defective.drop(defective.columns[0], axis=1)\n",
        "\n",
        "    y_test = defective[defective.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    defective['predict'] = y_pred\n",
        "    if effort == 20:\n",
        "      effort_instances =  (effort * defective[LOC].sum())/100\n",
        "      index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    elif effort == 1000:\n",
        "      index = defective[LOC].cumsum().searchsorted(effort)\n",
        "    else:\n",
        "      index = defective[LOC].cumsum().searchsorted(effort)\n",
        " \n",
        "    TargetList = defective[:index]\n",
        "\n",
        "    real_bugs = np.count_nonzero(y_test == 1)\n",
        "\n",
        "    PII = np.count_nonzero(TargetList['predict'] == 1)/ len(defective)\n",
        "    CE = np.count_nonzero(TargetList['predict'] == 1)/ real_bugs\n",
        "\n",
        "    if PII > 1.0:\n",
        "      PII = 1.0\n",
        "    if CE > 1.0:\n",
        "      CE = 1.0  \n",
        "    return PII, CE\n",
        "  \n",
        "  def _model_evaluating(self, model, scaler):\n",
        "    \n",
        "    cols = list(self.test.columns)\n",
        "    LOC = cols[1]\n",
        "\n",
        "    X_test = self.test.drop(self.test.columns[0], axis=1)\n",
        "    y_test = self.test[self.test.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "    \n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    test_data = self.test\n",
        "    test_data['score'] = y_proba\n",
        "    test_data['score*loc'] = test_data['score'] * test_data[LOC]\n",
        "    defective = test_data.loc[test_data[test_data.columns[0]] == 1]\n",
        "    defective = defective.sort_values(by='score*loc', ascending=False)\n",
        "    no_defective = test_data.loc[test_data[test_data.columns[0]] == 0]\n",
        "    no_defective = no_defective.sort_values(by='score*loc', ascending=False)\n",
        "\n",
        "    DEFECTIVE = pd.concat([defective, no_defective]).reset_index(drop=True)\n",
        "    DEFECTIVE = DEFECTIVE.replace({inf: 1.0})\n",
        "\n",
        "    X_test = DEFECTIVE.drop([DEFECTIVE.columns[0], 'score*loc', 'score'], axis=1)\n",
        "    y_test = DEFECTIVE[DEFECTIVE.columns[0]]\n",
        "\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    F1 = round(f1_score(y_test, y_pred), 5)\n",
        "    AUC = round(roc_auc_score(y_test, y_prob), 5)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    PF = round(fp / (fp + tn), 5)\n",
        "\n",
        "    NPM = [F1, AUC, PF, precision, recall, accuracy]\n",
        "\n",
        "    test_data = test_data.drop('score*loc', axis=1)\n",
        "\n",
        "    test_data['score/loc'] = test_data['score'] / test_data[LOC]\n",
        "    defective = test_data.loc[test_data[test_data.columns[0]] == 1]\n",
        "    defective = defective.sort_values(by='score/loc', ascending=False)\n",
        "    no_defective = test_data.loc[test_data[test_data.columns[0]] == 0]\n",
        "    no_defective = no_defective.sort_values(by='score/loc', ascending=False)\n",
        "    \n",
        "    DEFECTIVE = pd.concat([defective, no_defective]).reset_index(drop=True)\n",
        "    DEFECTIVE = DEFECTIVE.replace({inf: 1.0})\n",
        "\n",
        "    DEFECTIVE = DEFECTIVE.drop(['score/loc', 'score'], axis=1)\n",
        "\n",
        "    IFA =  self._calf_IFA(DEFECTIVE, LOC, model, scaler, 20)\n",
        "    PII20, CE20 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 20)\n",
        "    PII1000, CE1000 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 1000)\n",
        "    PII2000, CE2000 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 2000)\n",
        "    Popt = self._calc_popt(DEFECTIVE, LOC, 20) \n",
        "    EPM = [IFA, PII20, PII1000, PII2000, CE20, CE1000, CE2000, Popt]\n",
        "\n",
        "    return NPM, EPM\n",
        "  \n",
        "  def _model_building(self, ds,\n",
        "                     base_estimator,\n",
        "                     scaler,\n",
        "                     resample_strategy,\n",
        "                     dsel_size):\n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    DS: object (Default = None)\n",
        "        The dynamic selection technique to fit and predict the target project.\n",
        "        \n",
        "        If None, then the dynamic selection technique is a\n",
        "        :class:`~deslib.des.KNORAU`.\n",
        "\n",
        "    base_estimator : object or list of base estimatos (Default = None)\n",
        "        The base estimator used to generated the pool of classifiers. The base\n",
        "        base_estimator should support the technique \"predict_proba\".\n",
        "        \n",
        "        If None, then the base estimator is a :class:`GaussianNB` from sklearn\n",
        "        available on :class:`~sklearn.naive_bayes.GaussianNB`.\n",
        "\n",
        "    scaler: object or list of scaler algorithms (Default = None)\n",
        "        The scaler algorithm to transform features by scaling each\n",
        "        feature to a given range.\n",
        "\n",
        "    resample_strategy : {'over', 'under', None} (Default = None)\n",
        "        The algorithm to perform random sampling\n",
        "\n",
        "        - 'over' will use :class:`RandomOverSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomOverSampler`\n",
        "\n",
        "        - 'under' will use :class:`RandomUnderSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomUnderSampler`\n",
        "\n",
        "        - None, will not use algorithm to perform random sampling.   \n",
        "\n",
        "    Returns\n",
        "      -------\n",
        "      NPM : list\n",
        "          list with the array_npm values of non-effort-aware array_npm\n",
        "          measures.\n",
        "    \n",
        "    \"\"\"\n",
        "    train_data = self.train\n",
        "    X = train_data.drop(train_data.columns[0], axis=1)\n",
        "    y = train_data[train_data.columns[0]]\n",
        "\n",
        "    bugs = list(np.unique(y))[1:]\n",
        "    y = y.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X = scaler.fit_transform(X)\n",
        "\n",
        "    if dsel_size != None:\n",
        "      X_train, X_dsel, y_train, y_dsel = train_test_split(X, y, test_size=dsel_size)\n",
        "    else:\n",
        "      X_train, y_train = X, y\n",
        "      X_dsel, y_dsel = X, y \n",
        "\n",
        "    if resample_strategy not in ['over', 'smote', None]:\n",
        "      raise ValueError(\"Value input is incorrect. Accept only three values: {'over', 'under', None}.\")\n",
        "                        \n",
        "    if resample_strategy == 'over':\n",
        "      resample_strategy = RandomOverSampler()\n",
        "      X_train, y_train = resample_strategy.fit_resample(X_train, y_train)\n",
        "      X_dsel, y_dsel = resample_strategy.fit_resample(X_dsel, y_dsel)\n",
        "\n",
        "    elif resample_strategy == 'smote':\n",
        "      resample_strategy = SMOTE()\n",
        "      X_train, y_train = resample_strategy.fit_resample(X_train, y_train)\n",
        "      X_dsel, y_dsel = resample_strategy.fit_resample(X_dsel, y_dsel)\n",
        "\n",
        "    if base_estimator == None:\n",
        "      base_estimator = GaussianNB()\n",
        "    pool_classifiers = BaggingClassifier(base_estimator=base_estimator)\n",
        "    pool_classifiers.fit(X_train, y_train)\n",
        "    model = ds.set_params(pool_classifiers=pool_classifiers)\n",
        "    model.fit(X_dsel, y_dsel)\n",
        "    \n",
        "    return model, scaler\n",
        "  \n",
        "  def dynamic_prediction(self, dynamic_algorithm=None,\n",
        "                         base_estimator=None, preprocessing=None,\n",
        "                         resample_strategy=None,\n",
        "                         dsel_size=None):\n",
        "      \n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    DS: object (Default = None)\n",
        "        The dynamic selection technique to fit and predict the target project.\n",
        "        \n",
        "        If None, then the dynamic selection technique is a\n",
        "        :class:`~deslib.des.KNORAU`.\n",
        "\n",
        "    base_estimator : object or list of base estimatos (Default = None)\n",
        "        The base estimator used to generated the pool of classifiers. The base\n",
        "        base_estimator should support the technique \"predict_proba\".\n",
        "        \n",
        "        If None, then the base estimator is a :class:`GaussianNB` from sklearn\n",
        "        available on :class:`~sklearn.naive_bayes.GaussianNB`.\n",
        "\n",
        "    preprocessing: object or list of scaler algorithms (Default = None)\n",
        "        The scaler algorithm to transform features by scaling each\n",
        "        feature to a given range.\n",
        "\n",
        "    resample_strategy : {'over', 'under', None} (Default = None)\n",
        "        The algorithm to perform random sampling\n",
        "\n",
        "        - 'over' will use :class:`RandomOverSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomOverSampler`\n",
        "\n",
        "        - 'under' will use :class:`RandomUnderSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomUnderSampler`\n",
        "\n",
        "        - None, will not use algorithm to perform random sampling.\n",
        "\n",
        "    dsel_size : float (Default = None)\n",
        "        The strategy to division of training data into TRAIN and DSEL\n",
        "        \n",
        "        If float, should be between 0.2 and 0.5 and represent the proportion of\n",
        "        the training dataset to include in the DSEL split. If None, TRAIN and\n",
        "        DSEL will receive all instances of training data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    best_model : object\n",
        "        The best model to predict the target project.\n",
        "\n",
        "    best_scaler : object\n",
        "        The best scaler algorithm to transform features by scaling.\n",
        "\n",
        "    Note: if  best_scaler = None, then doesn't use any pre-processing algorithm.\n",
        "    \"\"\" \n",
        "\n",
        " \n",
        "\n",
        "    if dynamic_algorithm == None:\n",
        "      dynamic_algorithm = [KNORAU()]\n",
        "    \n",
        "    if type(base_estimator) != list and base_estimator != None:\n",
        "      base_estimator = [base_estimator]\n",
        "    elif base_estimator == None:\n",
        "      base_estimator = [LogisticRegression(solver='liblinear')]\n",
        "\n",
        "    if preprocessing == None:\n",
        "      preprocessing = [preprocessing]\n",
        "    elif type(preprocessing) != list and preprocessing != None:\n",
        "      preprocessing = [preprocessing]\n",
        "\n",
        "    if resample_strategy == None:\n",
        "      resample_strategy = [resample_strategy]\n",
        "    elif type(resample_strategy) != list and resample_strategy != None:\n",
        "      resample_strategy = [resample_strategy]\n",
        "\n",
        "    if (type(dsel_size) == float and dsel_size < 0.2) or (type(dsel_size) == float and dsel_size > 0.5):\n",
        "      raise ValueError('Value inputed for dsel_size is invalid. Accepts only float between 0.2 and 0.5 or None.')\n",
        "    elif dsel_size != None and type(dsel_size) != float:\n",
        "      raise ValueError('Value inputed for dsel is invalid. Accepts only float between 0.2 and 0.5 or None.')\n",
        "\n",
        "    NPM, EPM = [], []\n",
        "    projetos_preditos = ['szybkafucha', 'termoproject', 'tomcat', 'velocity-1.4', 'velocity-1.5', 'velocity-1.6', 'workflow', 'wspomaganiepi']\n",
        "    list_projects = list(np.unique(self.dataset_total['name']))\n",
        "    print(list_projects)\n",
        "    aux = []\n",
        "    performance_NPM, performance_EPM, = [], []\n",
        "    for target_project in list_projects:\n",
        "      \n",
        "      if target_project in projetos_preditos:\n",
        "        print(target_project)\n",
        "        for ds in dynamic_algorithm:\n",
        "          string_ds = str(type(ds))\n",
        "          string_ds = string_ds.split(\"'\")[1]\n",
        "          string_ds = string_ds.split(\".\")\n",
        "\n",
        "          name_ds = string_ds[len(string_ds)-1]\n",
        "          print(name_ds)\n",
        "          if 'deslib' not in string_ds:\n",
        "            raise ValueError('Input dynamic selection technique invalid!')\n",
        "          epm_data = []\n",
        "          for classifier in base_estimator:\n",
        "            for s in preprocessing:\n",
        "              for resampling in resample_strategy:\n",
        "                self._target_definition(target_project, self.dataset_total)\n",
        "                model, scaler = self._model_building(ds=ds, base_estimator=classifier,\n",
        "                                                    scaler=s, resample_strategy=None,\n",
        "                                                    dsel_size=dsel_size)\n",
        "                array_npm, array_epm = self._model_evaluating(model=model, scaler=scaler)\n",
        "                \n",
        "                array_npm.insert(0, self.dataset_name)\n",
        "                array_npm.insert(1, target_project)\n",
        "                array_npm.insert(2, self.percent_bugs)\n",
        "                array_npm.insert(3, name_ds)\n",
        "                array_npm.insert(4, scaler)\n",
        "        \n",
        "                cols = ['Dataset', 'Project', 'Percent_Bugs', 'DS', 'scaler',  'f1', 'auc', 'pf']\n",
        "                array_npm = pd.DataFrame([array_npm], columns=cols)         \n",
        "                performance_NPM.append(array_npm)\n",
        "                ds_data.append(array_npm)\n",
        "                aux.append(array_npm)\n",
        "\n",
        "                array_epm.insert(0, self.dataset_name)\n",
        "                array_epm.insert(1, target_project)\n",
        "                array_epm.insert(2, self.percent_bugs)\n",
        "                array_epm.insert(3, name_ds)\n",
        "                cols = ['Dataset', 'Project', 'Percent_Bugs', 'DS','IFA', 'PII20', 'PII1000', 'PII2000', 'CE20', 'CE1000', 'CE2000', 'Popt']\n",
        "                array_epm = pd.DataFrame([array_epm], columns=cols) \n",
        "                epm_data.append(array_epm)\n",
        "                performance_EPM.append(array_epm)\n",
        "          epm_data = pd.concat(epm_data).reset_index(drop=True)\n",
        "          \n",
        "    performance_NPM = pd.concat(performance_NPM).sort_values(by='Percent_Bugs').reset_index(drop=True)\n",
        "    dict_npm = dict()\n",
        "    for metric in list(['f1', 'auc', 'pf']):\n",
        "      array = []\n",
        "      project_bugs = []\n",
        "      for project in list(np.unique(performance_NPM['Project'])):\n",
        "        p_data = performance_NPM.loc[performance_NPM['Project'] == project]\n",
        "        metric_data = p_data[metric]\n",
        "        if metric == 'pf':\n",
        "          max = np.min(metric_data)\n",
        "        else:\n",
        "          max = np.max(metric_data)  \n",
        "        array.append(max)\n",
        "        aux = [project, list(np.unique(p_data['Percent_Bugs']))[0]]\n",
        "        project_bugs.append(pd.DataFrame([aux], columns=['Project', '%']))\n",
        "      dict_npm[metric] = array\n",
        "    project_bugs = pd.concat(project_bugs).reset_index(drop=True)\n",
        "\n",
        "    f1 = pd.DataFrame(dict_npm['f1'], columns=['f1'])\n",
        "    auc = pd.DataFrame(dict_npm['auc'], columns=['auc'])\n",
        "    pf = pd.DataFrame(dict_npm['pf'], columns=['pf'])\n",
        "\n",
        "    NPM = pd.concat([f1, auc, pf], axis=1)\n",
        "    NPM = pd.concat([project_bugs, NPM], axis=1).reindex(project_bugs.index)\n",
        "  \n",
        "    performance_EPM = pd.concat(performance_EPM).reset_index(drop=True)\n",
        "\n",
        "    dict_epm = dict()\n",
        "    for metric in list(['IFA', 'PII20', 'PII1000', 'PII2000', 'CE20', 'CE1000', 'CE2000', 'Popt']):\n",
        "      array = []\n",
        "      project_bugs = []\n",
        "      for project in list(np.unique(performance_EPM['Project'])):\n",
        "        p_data = performance_EPM.loc[performance_EPM['Project'] == project]\n",
        "        metric_data = p_data[metric]\n",
        "        if metric in ['IFA', 'PII20', 'PII1000', 'PII2000']:\n",
        "          max = np.min(metric_data)\n",
        "        else:\n",
        "          max = np.max(metric_data)  \n",
        "        array.append(max)\n",
        "        aux = [project, list(np.unique(p_data['Percent_Bugs']))[0]]\n",
        "        project_bugs.append(pd.DataFrame([aux], columns=['Project', '%']))\n",
        "      dict_epm[metric] = array\n",
        "    project_bugs = pd.concat(project_bugs).reset_index(drop=True)\n",
        "\n",
        "    IFA = pd.DataFrame(dict_epm['IFA'], columns=['IFA']).reset_index(drop=True)\n",
        "    PII20 = pd.DataFrame(dict_epm['PII20'], columns=['PII20']).reset_index(drop=True)\n",
        "    PII1000 = pd.DataFrame(dict_epm['PII1000'], columns=['PII1000']).reset_index(drop=True)\n",
        "    PII2000 = pd.DataFrame(dict_epm['PII2000'], columns=['PII2000']).reset_index(drop=True)\n",
        "    CE20 = pd.DataFrame(dict_epm['CE20'], columns=['CE20']).reset_index(drop=True)\n",
        "    CE1000 = pd.DataFrame(dict_epm['CE1000'], columns=['CE1000']).reset_index(drop=True)\n",
        "    CE2000 = pd.DataFrame(dict_epm['CE2000'], columns=['CE2000']).reset_index(drop=True)\n",
        "    Popt = pd.DataFrame(dict_epm['Popt'], columns=['Popt']).reset_index(drop=True)\n",
        "\n",
        "    EPM = pd.concat([IFA, PII20, PII1000, PII2000, CE20, CE1000, CE2000, Popt], axis=1).reindex(project_bugs.index)\n",
        "    EPM = pd.concat([project_bugs, EPM], axis=1).reindex(project_bugs.index)\n",
        "\n",
        "    return NPM, EPM"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
