{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSSC_estudo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16YhaiEaPUYHISURgSPe-6pRdJAyGOcib",
      "authorship_tag": "ABX9TyMry+lnL96BgWqOyCoDmryt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsaj/dssc/blob/master/DSSC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vs1UWSyphKD"
      },
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import re, sys, itertools, pickle\n",
        "\n",
        "from warnings import filterwarnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# base classifiers\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# import ds techniques\n",
        "from deslib.dcs import OLA, LCA, Rank, MCB\n",
        "from deslib.des import KNORAE, KNORAU, KNOP, METADES\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "from sklearn.metrics import f1_score, auc, roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "from scipy import *\n",
        "\n",
        "# from sklearn.utils.testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71WhOIGxRRJP"
      },
      "source": [
        "class DSSC():\n",
        "  \"\"\"Dynamic Selection Supervised Cross-project defect prediction (DSSC).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "  References\n",
        "    ----------\n",
        "\n",
        "    R. M. Cruz, L. G. Hafemann, R. Sabourin, and G. D. Cavalcanti.\n",
        "    \"Deslib:  A dynamicensemble selection library in python.\"\n",
        "    Journal of Machine Learning Research, vol. 21, no. 8, pp. 1–5, 2020.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, url_dataset, with_PF=True):\n",
        "\n",
        "    dataset_name = url_dataset.split('/')\n",
        "    self.dataset_name = dataset_name[len(dataset_name)-1]\n",
        "    self.url_dataset = url_dataset + '/*'\n",
        "    # self.pf_has_been_called = False\n",
        "\n",
        "    dataset_total = []\n",
        "\n",
        "    for project in glob(self.url_dataset):\n",
        "      project_name = project.split('/')\n",
        "      project_name = project_name[len(project_name)-1].split('.csv')[0]\n",
        "      ds = pd.read_csv(project)\n",
        "\n",
        "      if 'name' in list(ds.columns):\n",
        "        ds.pop('name')\n",
        "        ds['name'] = project_name\n",
        "      else:\n",
        "        ds['name'] = project_name\n",
        "      dataset_total.append(ds)\n",
        "    \n",
        "    dataset_total = pd.concat(dataset_total).reset_index(drop=True)\n",
        "\n",
        "    #project filtering stage\n",
        "    if with_PF == True:\n",
        "      dataset_total = self._project_filtering(dataset_total)\n",
        "\n",
        "    self.dataset_total = dataset_total\n",
        "    self.train, self.test = [], []\n",
        "    self.percent_bugs = 0\n",
        "  \n",
        "  def _project_filtering(self, dataset_total):\n",
        "\n",
        "    \"\"\"Filter of projects for prediction.\n",
        "\n",
        "    Each project and its versions are checked to see if they have a minimum\n",
        "    number of 5 instances of each label (defect and non-defect).\n",
        " \n",
        "    Parameters\n",
        "    ----------\n",
        "    with_PF : Boolean (Default = False)\n",
        "        Determines if the filter is applied to check if project have a minimum\n",
        "        number of instances.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    dataset_total : DataFrame\n",
        "        DataFrame containing all projects with minimum number of instances.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    S. Herbold,  A. Trautsch,  and J. Grabowski. \"A comparative study to\n",
        "    benchmark cross-project  defect  prediction  approaches\". IEEE  Transactions\n",
        "    on  Software  Engineering, vol. 44, no. 9, pp. 811–833, 2017.\n",
        "\n",
        "    \"\"\"\n",
        "    # self.pf_has_been_called = True\n",
        "    # for project in list(np.unique(dataset_total['name'])):\n",
        "    #   ds = dataset_total.loc[dataset_total['name'] == project]\n",
        "    #   bugs = list(np.unique(ds[ds.columns[0]]))[1:]\n",
        "    #   ds[ds.columns[0]] = ds[ds.columns[0]].replace(bugs, 1)\n",
        "    #   defective = np.count_nonzero(np.array(ds[ds.columns[0]]) == 1)\n",
        "    #   no_defective = np.count_nonzero(np.array(ds[ds.columns[0]]) == 0)\n",
        "\n",
        "    #   percent_bugs =  (defective / len(ds)) * 100\n",
        "    #   percent_no_bugs =  (no_defective / len(ds)) * 100\n",
        "\n",
        "    #   print(project, 'bugs: ', percent_bugs, ' | no_bugs: ', percent_no_bugs)\n",
        "    #   if len(ds) < 100:\n",
        "    #     dataset_total.drop(dataset_total.loc[dataset_total['name']==project].index, inplace=True)\n",
        "    #   elif len(ds) > 100 and percent_bugs < 5.0:\n",
        "    #     dataset_total.drop(dataset_total.loc[dataset_total['name']==project].index, inplace=True)\n",
        "    #   elif len(ds) > 100 and percent_no_bugs < 5.0:\n",
        "    #     dataset_total.drop(dataset_total.loc[dataset_total['name']==project].index, inplace=True)\n",
        "    # print('opa: ', list(np.unique(dataset_total['name'])))\n",
        "    return dataset_total\n",
        "\n",
        "  def _target_definition(self, target_project, dataset_total):\n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    \n",
        "    target_project : string\n",
        "        String with name of target project.\n",
        "\n",
        "    dataset_total : DataFrame\n",
        "        DataFrame containing all projects for prediction\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train : {DataFrame} of shape (n_samples, n_features)\n",
        "        The training input samples.\n",
        "\n",
        "    test : {DataFrame} of shape (n_samples, n_features)\n",
        "        The target project input samples.\n",
        "\n",
        "    \"\"\"\n",
        "    target_name = target_project.split('.csv')[0]\n",
        "\n",
        "    if '-' in target_name:\n",
        "      target_name = target_project.split('-')[0]\n",
        "\n",
        "    if '.' in target_name:\n",
        "      target_name = target_project.split('.')[0]\n",
        "\n",
        "    for character in target_name:\n",
        "      if character.isdigit():\n",
        "        target_name = list(re.findall(r'(\\w+?)(\\d+)', target_name)[0])\n",
        "        target_name = target_name[0]\n",
        "        break\n",
        "\n",
        "    test_data = dataset_total.loc[dataset_total['name'] == target_project]\n",
        "    test_data = test_data.select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "    bugs = list(np.unique(test_data[test_data.columns[0]]))[1:]\n",
        "    test_data[test_data.columns[0]] = test_data[test_data.columns[0]].replace(bugs, 1)\n",
        "    \n",
        "    train_data = dataset_total[~dataset_total['name'].str.contains(target_name)]\n",
        "    for project_name in list(np.unique(train_data['name'])):\n",
        "      ds = train_data.loc[train_data['name'] == project_name]\n",
        "      y = ds[ds.columns[0]]\n",
        "      bugs = list(np.unique(y))[1:]\n",
        "      y = y.replace(bugs, 1)\n",
        "      defective = round((np.count_nonzero(np.array(y) == 1) / len(y)) * 100, 2)\n",
        "      no_defective = round((np.count_nonzero(np.array(y) == 0) / len(y)) * 100, 2)\n",
        "      # print(project_name, ' || bugs: ', defective, ' | nobugs: ', no_defective)\n",
        "      if defective < 5.0 or no_defective < 5.0 or len(ds) < 100:\n",
        "        # print('Removido do train :', project_name)\n",
        "        train_data = train_data[train_data['name'] != project_name]\n",
        "    train_data = train_data.select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "    bugs = list(np.unique(train_data[train_data.columns[0]]))[1:]\n",
        "    train_data[train_data.columns[0]] = train_data[train_data.columns[0]].replace(bugs, 1)\n",
        "\n",
        "    def Diff(li1, li2):\n",
        "      return list(set(li1) - set(li2)) + list(set(li2) - set(li1))\n",
        "\n",
        "    diff_columns = Diff(list(train_data.columns), list(test_data.columns))\n",
        "    \n",
        "    if len(diff_columns) > 0:\n",
        "      for col in diff_columns:\n",
        "        if col in list(train_data.columns):\n",
        "          train_data.pop(col)\n",
        "        elif col in list(test_data.columns):\n",
        "          test_data.pop(col)\n",
        "\n",
        "   \n",
        "    self.test = test_data\n",
        "    self.train = train_data\n",
        "\n",
        "    y = self.test[self.test.columns[0]]\n",
        "    bugs = list(np.unique(y))[1:]\n",
        "    y = y.replace(bugs, 1)\n",
        "    self.percent_bugs = round((np.count_nonzero(np.array(y) == 1) / len(y)) * 100, 2)\n",
        "\n",
        "    # return train, test\n",
        "\n",
        "  def _calc_popt(self, defective, LOC, effort):\n",
        "    \n",
        "    effort_instances =  (effort * defective[LOC].sum())/100\n",
        "    index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    TargetList = defective[:index]\n",
        "    y_test = TargetList[TargetList.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1) \n",
        "    effort_percent = np.arange(0, 101, 1)\n",
        "    defective_list = []\n",
        "\n",
        "    for percent in range(0, 101):\n",
        "      effort_loc =  (percent * TargetList[LOC].sum())/100\n",
        "      index = TargetList[LOC].cumsum().searchsorted(effort_loc)\n",
        "      data = defective[:index]\n",
        "      \n",
        "      if len(data) != 0 and percent < 100:\n",
        "        bugs = np.count_nonzero(data[data.columns[0]] == 1)\n",
        "        percent_bugs = bugs / np.count_nonzero( y_test == 1)\n",
        "        defective_list.append(percent_bugs)\n",
        "      elif len(data) != 0 and percent == 100:\n",
        "        percent_bugs = 1.0\n",
        "        defective_list.append(percent_bugs)\n",
        "      else:\n",
        "        percent_bugs = 0.0\n",
        "        defective_list.append(percent_bugs)\n",
        "      \n",
        "    x = effort_percent\n",
        "    y = defective_list\n",
        "\n",
        "    y_a = np.arange(0.0, 1.01, 0.02)\n",
        "    x_b = np.arange(0, 101, 2)\n",
        "\n",
        "    x_a, y_b, h = [], [], []\n",
        "    for i in range(51):\n",
        "      x_a.append(0)\n",
        "      y_b.append(1)\n",
        "      h.append(100)\n",
        "\n",
        "    x1 = np.concatenate([x_a, x_b])\n",
        "    y1  = np.concatenate([y_a, y_b])\n",
        "\n",
        "    x2 = np.concatenate([x_b, h])\n",
        "    y2 = np.concatenate([x_a, y_a])\n",
        "\n",
        "    area_P_R = auc(x, y) - auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "    area_O_P = auc(x1, y1) - auc(x, y)\n",
        "    area_P_R = auc(x, y) - auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "    area_R_W = auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01)) - auc(x2, y2)\n",
        "\n",
        "    # plt.plot(x1, y1)\n",
        "    # plt.plot(x, y)\n",
        "    # plt.plot(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "    # plt.plot(x2, y2)\n",
        "    # plt.show()\n",
        "    popt = 1 - (area_O_P/ (area_O_P + area_P_R + area_R_W))\n",
        "    if popt > 1:\n",
        "      popt = 1.0\n",
        "    return popt\n",
        "  \n",
        "  def _calf_IFA(self, defective, LOC, model, scaler, effort):\n",
        "    X_test = defective.drop(defective.columns[0], axis=1)\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    defective['predict'] = y_pred\n",
        "    effort_instances =  (effort * defective[LOC].sum())/100\n",
        "    index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    TargetList = defective[:index]\n",
        "    y_test = TargetList[TargetList.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1) \n",
        "    \n",
        "    IFA = 0\n",
        "    for pred, true in zip(y_pred, y_test):\n",
        "      if true == 1 and pred == 1:\n",
        "        break\n",
        "      elif true == 0 and pred == 1:\n",
        "        IFA +=1\n",
        "\n",
        "    return IFA\n",
        "  \n",
        "  def _calc_PIIL_CEL(self, defective, LOC, model, scaler, effort):\n",
        "    \n",
        "    if 'predict' in list(defective.columns):\n",
        "      defective = defective.drop('predict', axis=1)\n",
        "    X_test = defective.drop(defective.columns[0], axis=1)\n",
        "\n",
        "    y_test = defective[defective.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    defective['predict'] = y_pred\n",
        "    if effort == 20:\n",
        "      effort_instances =  (effort * defective[LOC].sum())/100\n",
        "      index = defective[LOC].cumsum().searchsorted(effort_instances)\n",
        "    elif effort == 1000:\n",
        "      index = defective[LOC].cumsum().searchsorted(effort)\n",
        "    else:\n",
        "      index = defective[LOC].cumsum().searchsorted(effort)\n",
        " \n",
        "    TargetList = defective[:index]\n",
        "    \n",
        "    # print(np.sum(defective[LOC]), np.sum(TargetList[LOC]))\n",
        "    real_bugs = np.count_nonzero(y_test == 1)\n",
        "    # PII = len(TargetList)/len(defective)\n",
        "    PII = np.count_nonzero(TargetList['predict'] == 1)/ len(defective)\n",
        "\n",
        "    CE = np.count_nonzero(TargetList['predict'] == 1)/ real_bugs\n",
        "\n",
        "    if PII > 1.0:\n",
        "      PII = 1.0\n",
        "    if CE > 1.0:\n",
        "      CE = 1.0  \n",
        "    return PII, CE\n",
        "  \n",
        "  def _model_evaluating(self, model, scaler):\n",
        "    \n",
        "    cols = list(self.test.columns)\n",
        "    LOC = cols[1]\n",
        "\n",
        "    X_test = self.test.drop(self.test.columns[0], axis=1)\n",
        "    y_test = self.test[self.test.columns[0]]\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "    \n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    test_data = self.test\n",
        "    test_data['score'] = y_proba\n",
        "    test_data['score*loc'] = test_data['score'] * test_data[LOC]\n",
        "    defective = test_data.loc[test_data[test_data.columns[0]] == 1]\n",
        "    defective = defective.sort_values(by='score*loc', ascending=False)\n",
        "    no_defective = test_data.loc[test_data[test_data.columns[0]] == 0]\n",
        "    no_defective = no_defective.sort_values(by='score*loc', ascending=False)\n",
        "    # DEFECTIVE = test_data.sort_values(by='score*loc', ascending=False)\n",
        "\n",
        "    DEFECTIVE = pd.concat([defective, no_defective]).reset_index(drop=True)\n",
        "    DEFECTIVE = DEFECTIVE.replace({inf: 1.0})\n",
        "\n",
        "    X_test = DEFECTIVE.drop([DEFECTIVE.columns[0], 'score*loc', 'score'], axis=1)\n",
        "    y_test = DEFECTIVE[DEFECTIVE.columns[0]]\n",
        "\n",
        "    bugs = list(np.unique(y_test))[1:]\n",
        "    y_test = y_test.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    F1 = round(f1_score(y_test, y_pred), 5)\n",
        "    AUC = round(roc_auc_score(y_test, y_prob), 5)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    PF = round(fp / (fp + tn), 5)\n",
        "\n",
        "    # precision = precision_score(y_test, y_pred)\n",
        "    # recall = recall_score(y_test, y_pred)\n",
        "    # accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    NPM = [F1, AUC, PF, precision, recall, accuracy]\n",
        "\n",
        "    test_data = test_data.drop('score*loc', axis=1)\n",
        "\n",
        "    test_data['score/loc'] = test_data['score'] / test_data[LOC]\n",
        "    defective = test_data.loc[test_data[test_data.columns[0]] == 1]\n",
        "    defective = defective.sort_values(by='score/loc', ascending=False)\n",
        "    no_defective = test_data.loc[test_data[test_data.columns[0]] == 0]\n",
        "    no_defective = no_defective.sort_values(by='score/loc', ascending=False)\n",
        "    \n",
        "    DEFECTIVE = pd.concat([defective, no_defective]).reset_index(drop=True)\n",
        "    DEFECTIVE = DEFECTIVE.replace({inf: 1.0})\n",
        "\n",
        "    DEFECTIVE = DEFECTIVE.drop(['score/loc', 'score'], axis=1)\n",
        "    # y_test = DEFECTIVE[DEFECTIVE.columns[0]]\n",
        "    # bugs = list(np.unique(y_test))[1:]\n",
        "    # y_test = y_test.replace(bugs, 1) \n",
        "\n",
        "    IFA =  self._calf_IFA(DEFECTIVE, LOC, model, scaler, 20)\n",
        "    PII20, CE20 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 20)\n",
        "    PII1000, CE1000 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 1000)\n",
        "    PII2000, CE2000 = self._calc_PIIL_CEL(DEFECTIVE, LOC, model, scaler, 2000)\n",
        "    Popt = self._calc_popt(DEFECTIVE, LOC, 20) \n",
        "    EPM = [IFA, PII20, PII1000, PII2000, CE20, CE1000, CE2000, Popt]\n",
        "\n",
        "    return NPM, EPM\n",
        "  \n",
        "  def _model_building(self, ds,\n",
        "                     base_estimator,\n",
        "                     scaler,\n",
        "                     resample_strategy,\n",
        "                     dsel_size):\n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    DS: object (Default = None)\n",
        "        The dynamic selection technique to fit and predict the target project.\n",
        "        \n",
        "        If None, then the dynamic selection technique is a\n",
        "        :class:`~deslib.des.KNORAU`.\n",
        "\n",
        "    base_estimator : object or list of base estimatos (Default = None)\n",
        "        The base estimator used to generated the pool of classifiers. The base\n",
        "        base_estimator should support the technique \"predict_proba\".\n",
        "        \n",
        "        If None, then the base estimator is a :class:`GaussianNB` from sklearn\n",
        "        available on :class:`~sklearn.naive_bayes.GaussianNB`.\n",
        "\n",
        "    scaler: object or list of scaler algorithms (Default = None)\n",
        "        The scaler algorithm to transform features by scaling each\n",
        "        feature to a given range.\n",
        "\n",
        "    resample_strategy : {'over', 'under', None} (Default = None)\n",
        "        The algorithm to perform random sampling\n",
        "\n",
        "        - 'over' will use :class:`RandomOverSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomOverSampler`\n",
        "\n",
        "        - 'under' will use :class:`RandomUnderSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomUnderSampler`\n",
        "\n",
        "        - None, will not use algorithm to perform random sampling.   \n",
        "\n",
        "    Returns\n",
        "      -------\n",
        "      NPM : list\n",
        "          list with the array_npm values of non-effort-aware array_npm\n",
        "          measures.\n",
        "    \n",
        "    \"\"\"\n",
        "    train_data = self.train\n",
        "    X = train_data.drop(train_data.columns[0], axis=1)\n",
        "    y = train_data[train_data.columns[0]]\n",
        "\n",
        "    bugs = list(np.unique(y))[1:]\n",
        "    y = y.replace(bugs, 1)\n",
        "\n",
        "    if scaler != None:\n",
        "      X = scaler.fit_transform(X)\n",
        "\n",
        "    if dsel_size != None:\n",
        "      X_train, X_dsel, y_train, y_dsel = train_test_split(X, y, test_size=dsel_size)\n",
        "    else:\n",
        "      X_train, y_train = X, y\n",
        "      X_dsel, y_dsel = X, y \n",
        "\n",
        "    if resample_strategy not in ['over', 'smote', None]:\n",
        "      raise ValueError(\"Value input is incorrect. Accept only three values: {'over', 'under', None}.\")\n",
        "                        \n",
        "    if resample_strategy == 'over':\n",
        "      resample_strategy = RandomOverSampler()\n",
        "      X_train, y_train = resample_strategy.fit_resample(X_train, y_train)\n",
        "      X_dsel, y_dsel = resample_strategy.fit_resample(X_dsel, y_dsel)\n",
        "\n",
        "    elif resample_strategy == 'smote':\n",
        "      resample_strategy = SMOTE()\n",
        "      X_train, y_train = resample_strategy.fit_resample(X_train, y_train)\n",
        "      X_dsel, y_dsel = resample_strategy.fit_resample(X_dsel, y_dsel)\n",
        "\n",
        "    if base_estimator == None:\n",
        "      base_estimator = GaussianNB()\n",
        "    pool_classifiers = BaggingClassifier(base_estimator=base_estimator)\n",
        "    pool_classifiers.fit(X_train, y_train)\n",
        "    model = ds.set_params(pool_classifiers=pool_classifiers)\n",
        "    model.fit(X_dsel, y_dsel)\n",
        "    \n",
        "    return model, scaler\n",
        "  \n",
        "  def dynamic_prediction(self, dynamic_algorithm=None,\n",
        "                         base_estimator=None, preprocessing=None,\n",
        "                         resample_strategy=None,\n",
        "                         dsel_size=None):\n",
        "      \n",
        "    \"\"\" Selecting the best model to predict the target project\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    DS: object (Default = None)\n",
        "        The dynamic selection technique to fit and predict the target project.\n",
        "        \n",
        "        If None, then the dynamic selection technique is a\n",
        "        :class:`~deslib.des.KNORAU`.\n",
        "\n",
        "    base_estimator : object or list of base estimatos (Default = None)\n",
        "        The base estimator used to generated the pool of classifiers. The base\n",
        "        base_estimator should support the technique \"predict_proba\".\n",
        "        \n",
        "        If None, then the base estimator is a :class:`GaussianNB` from sklearn\n",
        "        available on :class:`~sklearn.naive_bayes.GaussianNB`.\n",
        "\n",
        "    preprocessing: object or list of scaler algorithms (Default = None)\n",
        "        The scaler algorithm to transform features by scaling each\n",
        "        feature to a given range.\n",
        "\n",
        "    resample_strategy : {'over', 'under', None} (Default = None)\n",
        "        The algorithm to perform random sampling\n",
        "\n",
        "        - 'over' will use :class:`RandomOverSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomOverSampler`\n",
        "\n",
        "        - 'under' will use :class:`RandomUnderSampler` from imblearn\n",
        "          available on :class:`~imblearn.underesample_strategyampling.RandomUnderSampler`\n",
        "\n",
        "        - None, will not use algorithm to perform random sampling.\n",
        "\n",
        "    dsel_size : float (Default = None)\n",
        "        The strategy to division of training data into TRAIN and DSEL\n",
        "        \n",
        "        If float, should be between 0.2 and 0.5 and represent the proportion of\n",
        "        the training dataset to include in the DSEL split. If None, TRAIN and\n",
        "        DSEL will receive all instances of training data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    best_model : object\n",
        "        The best model to predict the target project.\n",
        "\n",
        "    best_scaler : object\n",
        "        The best scaler algorithm to transform features by scaling.\n",
        "\n",
        "    Note: if  best_scaler = None, then doesn't use any pre-processing algorithm.\n",
        "    \"\"\" \n",
        "\n",
        " \n",
        "\n",
        "    if dynamic_algorithm == None:\n",
        "      dynamic_algorithm = [KNORAU()]\n",
        "    \n",
        "    if type(base_estimator) != list and base_estimator != None:\n",
        "      base_estimator = [base_estimator]\n",
        "    elif base_estimator == None:\n",
        "      base_estimator = [LogisticRegression(solver='liblinear')]\n",
        "\n",
        "    if preprocessing == None:\n",
        "      preprocessing = [preprocessing]\n",
        "    elif type(preprocessing) != list and preprocessing != None:\n",
        "      preprocessing = [preprocessing]\n",
        "\n",
        "    if resample_strategy == None:\n",
        "      resample_strategy = [resample_strategy]\n",
        "    elif type(resample_strategy) != list and resample_strategy != None:\n",
        "      resample_strategy = [resample_strategy]\n",
        "\n",
        "    if (type(dsel_size) == float and dsel_size < 0.2) or (type(dsel_size) == float and dsel_size > 0.5):\n",
        "      raise ValueError('Value inputed for dsel_size is invalid. Accepts only float between 0.2 and 0.5 or None.')\n",
        "    elif dsel_size != None and type(dsel_size) != float:\n",
        "      raise ValueError('Value inputed for dsel is invalid. Accepts only float between 0.2 and 0.5 or None.')\n",
        "\n",
        "    NPM, EPM = [], []\n",
        "    projetos_preditos = ['szybkafucha', 'termoproject', 'tomcat', 'velocity-1.4', 'velocity-1.5', 'velocity-1.6', 'workflow', 'wspomaganiepi']\n",
        "    list_projects = list(np.unique(self.dataset_total['name']))\n",
        "    print(list_projects)\n",
        "    aux = []\n",
        "    performance_NPM, performance_EPM, = [], []\n",
        "    for target_project in list_projects:\n",
        "      \n",
        "      if target_project in projetos_preditos:\n",
        "        print(target_project)\n",
        "        for ds in dynamic_algorithm:\n",
        "          string_ds = str(type(ds))\n",
        "          string_ds = string_ds.split(\"'\")[1]\n",
        "          string_ds = string_ds.split(\".\")\n",
        "\n",
        "          name_ds = string_ds[len(string_ds)-1]\n",
        "          print(name_ds)\n",
        "          if 'deslib' not in string_ds:\n",
        "            raise ValueError('Input dynamic selection technique invalid!')\n",
        "          epm_data = []\n",
        "          for classifier in base_estimator:\n",
        "            for s in preprocessing:\n",
        "              for resampling in resample_strategy:\n",
        "                self._target_definition(target_project, self.dataset_total)\n",
        "                model, scaler = self._model_building(ds=ds, base_estimator=classifier,\n",
        "                                                    scaler=s, resample_strategy=None,\n",
        "                                                    dsel_size=dsel_size)\n",
        "                array_npm, array_epm = self._model_evaluating(model=model, scaler=scaler)\n",
        "                \n",
        "                array_npm.insert(0, self.dataset_name)\n",
        "                array_npm.insert(1, target_project)\n",
        "                array_npm.insert(2, self.percent_bugs)\n",
        "                array_npm.insert(3, name_ds)\n",
        "                array_npm.insert(4, scaler)\n",
        "                # array_npm.insert(5, resampling)\n",
        "                cols = ['Dataset', 'Project', 'Percent_Bugs', 'DS', 'scaler',  'f1', 'auc', 'pf']\n",
        "                array_npm = pd.DataFrame([array_npm], columns=cols)         \n",
        "                performance_NPM.append(array_npm)\n",
        "                ds_data.append(array_npm)\n",
        "                aux.append(array_npm)\n",
        "\n",
        "                array_epm.insert(0, self.dataset_name)\n",
        "                array_epm.insert(1, target_project)\n",
        "                array_epm.insert(2, self.percent_bugs)\n",
        "                array_epm.insert(3, name_ds)\n",
        "                array_epm.insert(4, s)\n",
        "                array_epm.insert(5, resampling)\n",
        "                cols = ['Dataset', 'Project', 'Percent_Bugs', 'DS', 'scaler', 'resampling','IFA', 'PII20', 'PII1000', 'PII2000', 'CE20', 'CE1000', 'CE2000', 'Popt']\n",
        "                array_epm = pd.DataFrame([array_epm], columns=cols) \n",
        "                epm_data.append(array_epm)\n",
        "                performance_EPM.append(array_epm)\n",
        "          epm_data = pd.concat(epm_data).reset_index(drop=True)\n",
        "          # epm_data.to_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/EPM/{}/{}_EPM_{}_{}.csv'.format(self.dataset_name, self.dataset_name, name_ds, target_project), index=False)\n",
        "\n",
        "    performance_NPM = pd.concat(performance_NPM).sort_values(by='Percent_Bugs').reset_index(drop=True)\n",
        "    dict_npm = dict()\n",
        "    for metric in list(['f1', 'auc', 'pf']):\n",
        "      array = []\n",
        "      project_bugs = []\n",
        "      for project in list(np.unique(performance_NPM['Project'])):\n",
        "        p_data = performance_NPM.loc[performance_NPM['Project'] == project]\n",
        "        metric_data = p_data[metric]\n",
        "        if metric == 'pf':\n",
        "          max = np.min(metric_data)\n",
        "        else:\n",
        "          max = np.max(metric_data)  \n",
        "        array.append(max)\n",
        "        aux = [project, list(np.unique(p_data['Percent_Bugs']))[0]]\n",
        "        project_bugs.append(pd.DataFrame([aux], columns=['Project', '%']))\n",
        "      dict_npm[metric] = array\n",
        "    project_bugs = pd.concat(project_bugs).reset_index(drop=True)\n",
        "\n",
        "    f1 = pd.DataFrame(dict_npm['f1'], columns=['f1'])\n",
        "    auc = pd.DataFrame(dict_npm['auc'], columns=['auc'])\n",
        "    pf = pd.DataFrame(dict_npm['pf'], columns=['pf'])\n",
        "    # precision = pd.DataFrame(dict_npm['precision'], columns=['precision'])\n",
        "    # recall = pd.DataFrame(dict_npm['recall'], columns=['recall'])\n",
        "    # accuracy = pd.DataFrame(dict_npm['accuracy'], columns=['accuracy'])\n",
        "    \n",
        "    # projects = list(np.unique(NPM['Project']))\n",
        "    NPM = pd.concat([f1, auc, pf, precision, recall, accuracy], axis=1)\n",
        "    # NPM.insert(0, 'Project', projects)\n",
        "\n",
        "    NPM = pd.concat([project_bugs, NPM], axis=1).reindex(project_bugs.index)\n",
        "  \n",
        "    performance_EPM = pd.concat(performance_EPM).reset_index(drop=True)\n",
        "\n",
        "    dict_epm = dict()\n",
        "    for metric in list(['IFA', 'PII20', 'PII1000', 'PII2000', 'CE20', 'CE1000', 'CE2000', 'Popt']):\n",
        "      array = []\n",
        "      project_bugs = []\n",
        "      for project in list(np.unique(performance_EPM['Project'])):\n",
        "        p_data = performance_EPM.loc[performance_EPM['Project'] == project]\n",
        "        metric_data = p_data[metric]\n",
        "        if metric in ['IFA', 'PII20', 'PII1000', 'PII2000']:\n",
        "          max = np.min(metric_data)\n",
        "        else:\n",
        "          max = np.max(metric_data)  \n",
        "        array.append(max)\n",
        "        aux = [project, list(np.unique(p_data['Percent_Bugs']))[0]]\n",
        "        project_bugs.append(pd.DataFrame([aux], columns=['Project', '%']))\n",
        "      dict_epm[metric] = array\n",
        "    project_bugs = pd.concat(project_bugs).reset_index(drop=True)\n",
        "\n",
        "    IFA = pd.DataFrame(dict_epm['IFA'], columns=['IFA']).reset_index(drop=True)\n",
        "    PII20 = pd.DataFrame(dict_epm['PII20'], columns=['PII20']).reset_index(drop=True)\n",
        "    PII1000 = pd.DataFrame(dict_epm['PII1000'], columns=['PII1000']).reset_index(drop=True)\n",
        "    PII2000 = pd.DataFrame(dict_epm['PII2000'], columns=['PII2000']).reset_index(drop=True)\n",
        "    CE20 = pd.DataFrame(dict_epm['CE20'], columns=['CE20']).reset_index(drop=True)\n",
        "    CE1000 = pd.DataFrame(dict_epm['CE1000'], columns=['CE1000']).reset_index(drop=True)\n",
        "    CE2000 = pd.DataFrame(dict_epm['CE2000'], columns=['CE2000']).reset_index(drop=True)\n",
        "    Popt = pd.DataFrame(dict_epm['Popt'], columns=['Popt']).reset_index(drop=True)\n",
        "\n",
        "    EPM = pd.concat([IFA, PII20, PII1000, PII2000, CE20, CE1000, CE2000, Popt], axis=1).reindex(project_bugs.index)\n",
        "    EPM = pd.concat([project_bugs, EPM], axis=1).reindex(project_bugs.index)\n",
        "\n",
        "    return NPM, EPM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDZNRrkF1vIb",
        "outputId": "9ab70725-017e-440f-ba14-d1493a2ac9da"
      },
      "source": [
        "!pip install deslib\n",
        "!git clone https://github.com/jsaj/dssc.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deslib\n",
            "  Downloading DESlib-0.3.5-py3-none-any.whl (158 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 158 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->deslib) (1.0.1)\n",
            "Installing collected packages: deslib\n",
            "Successfully installed deslib-0.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz2rlmtyJkBE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uoa3d1uyWLMS",
        "outputId": "9b28fabd-8efd-40ad-c31b-39313cd62856"
      },
      "source": [
        "filterwarnings('ignore')\n",
        "# '/content/ml/Datasets/AEEEM',\n",
        "#             '/content/ml/Datasets/NASA',\n",
        "datasets = ['/content/ml/Datasets/PROMISE']\n",
        "\n",
        "# dynamic_algorithm = [KNORAE()]\n",
        "dynamic_algorithm = [KNORAE(), KNORAU(), KNOP(), METADES(), LCA(), OLA(), MCB(), Rank()]\n",
        "\n",
        "base_estimator = [LogisticRegression(solver='liblinear'),\n",
        "                  RandomForestClassifier(),\n",
        "                  GaussianNB(),\n",
        "                  DecisionTreeClassifier()]\n",
        "\n",
        "preprocessing = [None,\n",
        "                 MinMaxScaler(),\n",
        "                 StandardScaler()]\n",
        "\n",
        "resample_strategy = [None,\n",
        "                    'over']\n",
        "\n",
        "for data in datasets:\n",
        "  NPM, EPM = [], []\n",
        "  dataset_name = data.split('/')\n",
        "  dataset_name = dataset_name[len(dataset_name)-1]\n",
        "  model = DSSC(data, with_PF=True)\n",
        "  \n",
        "\n",
        "  npm, epm = model.dynamic_prediction(dynamic_algorithm=dynamic_algorithm,\n",
        "                                 base_estimator=base_estimator,\n",
        "                                 preprocessing=preprocessing,\n",
        "                                 resample_strategy=resample_strategy)\n",
        "  \n",
        "  # npm.to_csv('/content/sample_data/DSSC-{}.csv'.format(dataset_name))\n",
        "  # print('F1: ', round(np.mean(npm['f1']), 3))\n",
        "  # print('AUC: ', round(np.mean(npm['auc']), 3))\n",
        "  # print('PF: ', round(np.mean(npm['pf']), 3))\n",
        "  # print('Precision: ', round(np.mean(npm['precision']), 3))\n",
        "  # print('Recall: ', round(np.mean(npm['recall']), 3))\n",
        "  # print('Accuracy: ', round(np.mean(npm['accuracy']), 3))\n",
        "\n",
        "  # print()\n",
        "\n",
        "  print('IFA: ', round(np.sum(epm['IFA']), 3))\n",
        "  print('PII20: ', round(np.mean(epm['PII20']), 3))\n",
        "  print('PII1000: ', round(np.mean(epm['PII1000']), 3))\n",
        "  print('PII2000: ', round(np.mean(epm['PII2000']), 3))\n",
        "  print('CE20: ', round(np.mean(epm['CE20']), 3))\n",
        "  print('CE1000: ', round(np.mean(epm['CE1000']), 3))\n",
        "  print('CE2000: ', round(np.mean(epm['CE2000']), 3))\n",
        "  print('Popt: ', round(np.mean(epm['Popt']), 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ant-1.3', 'ant-1.4', 'ant-1.5', 'ant-1.6', 'ant-1.7', 'arc', 'berek', 'camel-1.0', 'camel-1.2', 'camel-1.4', 'camel-1.6', 'ckjm', 'e_learning', 'forrest-0.7', 'ivy-1.1', 'ivy-1.4', 'ivy-2.0', 'jedit-3.2', 'jedit-4.0', 'jedit-4.1', 'jedit-4.2', 'jedit-4.3', 'kalkulator', 'log4j-1.0', 'log4j-1.1', 'log4j-1.2', 'lucene-2.0', 'lucene-2.2', 'lucene-2.4', 'nieruchomosci', 'pbeans-1', 'pbeans-2', 'pdftranslator', 'poi-1.5', 'poi-2.0', 'poi-2.5', 'poi-3.0', 'redaktor', 'serapion', 'skarbonka', 'sklebagd', 'synapse-1.0', 'synapse-1.1', 'synapse-1.2', 'systemdata', 'szybkafucha', 'termoproject', 'tomcat', 'velocity-1.4', 'velocity-1.5', 'velocity-1.6', 'workflow', 'wspomaganiepi', 'xalan-2.4', 'xalan-2.5', 'xalan-2.6', 'xalan-2.7', 'xerces-1.2', 'xerces-1.3', 'xerces-1.4', 'xerces-init', 'zuzel']\n",
            "szybkafucha\n",
            "KNORAE\n",
            "KNORAU\n",
            "KNOP\n",
            "METADES\n",
            "LCA\n",
            "OLA\n",
            "MCB\n",
            "Rank\n",
            "termoproject\n",
            "KNORAE\n",
            "KNORAU\n",
            "KNOP\n",
            "METADES\n",
            "LCA\n",
            "OLA\n",
            "MCB\n",
            "Rank\n",
            "tomcat\n",
            "KNORAE\n",
            "KNORAU\n",
            "KNOP\n",
            "METADES\n",
            "LCA\n",
            "OLA\n",
            "MCB\n",
            "Rank\n",
            "velocity-1.4\n",
            "KNORAE\n",
            "KNORAU\n",
            "KNOP\n",
            "METADES\n",
            "LCA\n",
            "OLA\n",
            "MCB\n",
            "Rank\n",
            "velocity-1.5\n",
            "KNORAE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-acd0796e3641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                  \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                  \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                  resample_strategy=resample_strategy)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# npm.to_csv('/content/sample_data/DSSC-{}.csv'.format(dataset_name))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b03456d83898>\u001b[0m in \u001b[0;36mdynamic_prediction\u001b[0;34m(self, dynamic_algorithm, base_estimator, preprocessing, resample_strategy, dsel_size)\u001b[0m\n\u001b[1;32m    558\u001b[0m                                                     \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                                                     dsel_size=dsel_size)\n\u001b[0;32m--> 560\u001b[0;31m                 \u001b[0marray_npm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_epm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# array_npm.insert(0, self.dataset_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b03456d83898>\u001b[0m in \u001b[0;36m_model_evaluating\u001b[0;34m(self, model, scaler)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0mIFA\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calf_IFA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mPII20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_PIIL_CEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mPII1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE1000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_PIIL_CEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mPII2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCE2000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_PIIL_CEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mPopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_popt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b03456d83898>\u001b[0m in \u001b[0;36m_calc_PIIL_CEL\u001b[0;34m(self, defective, LOC, model, scaler, effort)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mdefective\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deslib/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mbase_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mbase_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mall_agree_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_classifier_agree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deslib/base.py\u001b[0m in \u001b[0;36m_predict_base\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_classifiers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_base_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    667\u001b[0m             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n\u001b[1;32m    668\u001b[0m                                             lock)\n\u001b[0;32m--> 669\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mcomplains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mthere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \"\"\"\n\u001b[0;32m--> 949\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is a class, not an instance.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36misclass\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0m__doc__\u001b[0m         \u001b[0mdocumentation\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         __module__      name of module in which this class was defined\"\"\"\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Hh3J_SnmoCtx",
        "outputId": "7938a537-3803-4a06-f75d-38a4a4279c26"
      },
      "source": [
        "data = []\n",
        "for csv in glob('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/EPM/  /*'):\n",
        "  data.append(pd.read_csv(csv))\n",
        "\n",
        "performance_EPM = pd.concat(data).reset_index(drop=True)\n",
        "# minmax = performance_EPM.loc[performance_EPM['scaler'] == 'MinMaxScaler()']\n",
        "# stantard =  performance_EPM.loc[performance_EPM['scaler'] == 'StandardScaler()']\n",
        "# performance_EPM = pd.concat([minmax, stantard])\n",
        "# performance_EPM = performance_EPM.loc[performance_EPM['resampling'] != 'over']\n",
        "# print(performance_EPM['resampling'])\n",
        "dict_epm = dict()\n",
        "for metric in list(['IFA', 'PII20', 'PII1000', 'PII2000', 'CE20', 'CE1000', 'CE2000', 'Popt']):\n",
        "  array = []\n",
        "  project_bugs = []\n",
        "  for project in list(np.unique(performance_EPM['Project'])):\n",
        "    p_data = performance_EPM.loc[performance_EPM['Project'] == project]\n",
        "    metric_data = p_data[metric]\n",
        "    if metric in ['IFA', 'PII20', 'PII1000', 'PII2000']:\n",
        "      max = np.min(metric_data)\n",
        "    else:\n",
        "      max = np.max(metric_data)  \n",
        "    array.append(max)\n",
        "    aux = [project, list(np.unique(p_data['Percent_Bugs']))[0]]\n",
        "    project_bugs.append(pd.DataFrame([aux], columns=['Project', '%']))\n",
        "  dict_epm[metric] = array\n",
        "project_bugs = pd.concat(project_bugs).reset_index(drop=True)\n",
        "\n",
        "\n",
        "IFA = pd.DataFrame(dict_epm['IFA'], columns=['IFA']).reset_index(drop=True)\n",
        "PII20 = pd.DataFrame(dict_epm['PII20'], columns=['PII20']).reset_index(drop=True)\n",
        "PII1000 = pd.DataFrame(dict_epm['PII1000'], columns=['PII1000']).reset_index(drop=True)\n",
        "PII2000 = pd.DataFrame(dict_epm['PII2000'], columns=['PII2000']).reset_index(drop=True)\n",
        "CE20 = pd.DataFrame(dict_epm['CE20'], columns=['CE20']).reset_index(drop=True)\n",
        "CE1000 = pd.DataFrame(dict_epm['CE1000'], columns=['CE1000']).reset_index(drop=True)\n",
        "CE2000 = pd.DataFrame(dict_epm['CE2000'], columns=['CE2000']).reset_index(drop=True)\n",
        "Popt = pd.DataFrame(dict_epm['Popt'], columns=['Popt']).reset_index(drop=True)\n",
        "\n",
        "epm = pd.concat([IFA, PII20, PII1000, PII2000, CE20, CE1000, CE2000, Popt], axis=1).reindex(project_bugs.index)\n",
        "epm = pd.concat([project_bugs, epm], axis=1).reindex(project_bugs.index)\n",
        "\n",
        "print('IFA: ', round(np.sum(epm['IFA']), 3))\n",
        "print('PII20: ', round(np.mean(epm['PII20']), 3))\n",
        "print('PII1000: ', round(np.mean(epm['PII1000']), 3))\n",
        "print('PII2000: ', round(np.mean(epm['PII2000']), 3))\n",
        "print('CE20: ', round(np.mean(epm['CE20']), 3))\n",
        "print('CE1000: ', round(np.mean(epm['CE1000']), 3))\n",
        "print('CE2000: ', round(np.mean(epm['CE2000']), 3))\n",
        "print('Popt: ', round(np.mean(epm['Popt']), 3))\n",
        "\n",
        "\n",
        "# IFA: 0\n",
        "# PII20:  0.052\n",
        "# PII1000:  0.003\n",
        "# PII2000:  0.009\n",
        "# CE20:  0.465\n",
        "# CE1000:  0.302\n",
        "# CE2000:  0.408\n",
        "# Popt:  0.724\n",
        "\n",
        "# IFA:  0\n",
        "# PII20:  0.052\n",
        "# PII1000:  0.003\n",
        "# PII2000:  0.009\n",
        "# CE20:  0.613\n",
        "# CE1000:  0.429\n",
        "# CE2000:  0.57\n",
        "# Popt:  0.731\n",
        "\n",
        "# IFA:  0\n",
        "# PII20:  0.021\n",
        "# PII1000:  0.0\n",
        "# PII2000:  0.001\n",
        "# CE20:  0.575\n",
        "# CE1000:  0.138\n",
        "# CE2000:  0.185\n",
        "# Popt:  0.747"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f6a092a7bffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mperformance_EPM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# minmax = performance_EPM.loc[performance_EPM['scaler'] == 'MinMaxScaler()']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# stantard =  performance_EPM.loc[performance_EPM['scaler'] == 'StandardScaler()']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4CUsAKRTlj3"
      },
      "source": [
        "ds = pd.read_csv('/content/sample_data/DSSC-NASA.csv')\n",
        "f1, auc, pf = [], [], []\n",
        "for p in list(np.unique(ds['Project'])):\n",
        "  data_p = ds.loc[ds['Project'] == p]\n",
        "  f1_max = np.max(data_p['f1'])\n",
        "  value = data_p.loc[data_p['f1'] == f1_max].reset_index(drop=True)\n",
        "  if len(value) > 1:\n",
        "    value = value[:1]\n",
        "  f1.append(value['f1'].item())\n",
        "\n",
        "  auc_max = np.max(data_p['auc'])\n",
        "  value = data_p.loc[data_p['auc'] == auc_max].reset_index(drop=True)\n",
        "  if len(value) > 1:\n",
        "    value = value[:1]\n",
        "  auc.append(value['auc'].item())\n",
        "\n",
        "  pf_max = np.min(data_p['pf'])\n",
        "  value = data_p.loc[data_p['pf'] == pf_max].reset_index(drop=True)\n",
        "  if len(value) > 1:\n",
        "    value = value[:1]\n",
        "  pf.append(value['pf'].item())\n",
        "\n",
        "print('f1: ', round(np.mean(f1), 3))\n",
        "print('auc: ', round(np.mean(auc), 3))\n",
        "print('pf: ', round(np.mean(pf), 3))\n",
        "\n",
        "#remove < 100\n",
        "# f1:  0.226\n",
        "# auc:  0.729\n",
        "# pf:  0.015"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O6oUaSpxb27"
      },
      "source": [
        "dataset = []\n",
        "for csv in glob('/content/sample_data/*'):\n",
        "  if 'AEEEM' in csv:\n",
        "    name_csv = csv.split('/')[3]\n",
        "    name_csv = name_csv.split('_')[3]\n",
        "    name_csv = name_csv.split('.csv')[0]\n",
        "    ds = pd.read_csv(csv)\n",
        "    ds['name'] = name_csv\n",
        "    dataset.append(ds)\n",
        "dataset = pd.concat(dataset).reset_index(drop=True)\n",
        "dataset\n",
        "\n",
        "for projeto in list(['RELINK']):\n",
        "  print('------- {} --------'.format(projeto))\n",
        "  projeto_values = dataset\n",
        "  \n",
        "  f1, auc, pf = [], [], []\n",
        "  for p in list(np.unique(projeto_values['Project'])):\n",
        "    array_DS = projeto_values.loc[projeto_values['Project'] == p]\n",
        "    max_f1 = np.max(array_DS['f1'])\n",
        "    best_f1 = array_DS.loc[array_DS['f1'] == max_f1].reset_index(drop=True)\n",
        "    if len(best_f1)> 1:\n",
        "      best_f1 = best_f1[:1]\n",
        "    f1.append(best_f1['f1'].item())\n",
        "\n",
        "    max_auc = np.max(array_DS['auc'])\n",
        "    best_auc = array_DS.loc[array_DS['auc'] == max_auc].reset_index(drop=True)\n",
        "    if len(best_auc)> 1:\n",
        "      best_auc = best_auc[:1]\n",
        "    auc.append(best_auc['auc'].item())\n",
        "\n",
        "    min_pf = np.min(array_DS['pf'])\n",
        "    best_pf = array_DS.loc[array_DS['pf'] == min_pf].reset_index(drop=True)\n",
        "    if len(best_pf)> 1:\n",
        "      best_pf = best_pf[:1]\n",
        "    pf.append(best_pf['pf'].item())\n",
        "\n",
        "  f1_mean, f1_var = round(np.mean(f1), 3),round(np.var(f1), 3)\n",
        "  auc_mean, auc_var = round(np.mean(auc), 3), round(np.var(auc), 3)\n",
        "  pf_mean, pf_var = round(np.mean(pf), 3), round(np.var(pf), 3)\n",
        "  array = [projeto,\n",
        "           str(f1_mean) + str('$\\pm$') + str(f1_var),\n",
        "           str(auc_mean) + str('$\\pm$') + str(auc_var),\n",
        "           str(pf_mean) + str('$\\pm$') + str(pf_var)]\n",
        "\n",
        "  res = pd.DataFrame([array], columns=['Project', 'F1-score', 'AUC', 'False Alarm'])\n",
        "  print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5moyAYvJDx8"
      },
      "source": [
        "path = '/content/sample_data/*'\n",
        "relink = []\n",
        "for i in glob(path):\n",
        "  if 'RELINK' in i:\n",
        "    ds = pd.read_csv(i) \n",
        "    relink.append(ds)\n",
        "relink = pd.concat(relink).reset_index(drop= True)\n",
        "relink.to_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/RELINK.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdM1ixV-rbgS"
      },
      "source": [
        "res = []\n",
        "for project in  list(np.unique(relink['Project'])):\n",
        "  data_project = relink.loc[relink['Project'] == project]\n",
        "  max_acc = np.max(data_project['acc'])\n",
        "  best_model = data_project.loc[data_project['acc'] == max_acc]\n",
        "  if len(best_model) > 1:\n",
        "    min = np.min(best_model['bsl'])\n",
        "    best_model = best_model.loc[best_model['bsl'] == min].reset_index(drop=True)\n",
        "    if len(best_model) > 1:\n",
        "      best_model = best_model[:1]\n",
        "  res.append(best_model)\n",
        "res = pd.concat(res).reset_index(drop=True)\n",
        "\n",
        "print('F1: ', round(res['f1'].mean(), 2))\n",
        "print('auc: ', round(res['auc'].mean(), 2))\n",
        "print('pf: ', round(res['pf'].mean(), 2))\n",
        "# res.to_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Estatistica/DSSC_{}_by_project.csv'.format(name_data), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQJXX1HA_qz"
      },
      "source": [
        "from collections import Counter, OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-i4wK2LSEYN"
      },
      "source": [
        "# ds = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/DSSC_dictory_NPM.csv')\n",
        "\n",
        "results = dict()\n",
        "results['promise'] = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/DSSC_PROMISE_NPM.csv')\n",
        "results['relink'] = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/DSSC_RELINK_NPM.csv')\n",
        "results['aeeem'] = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/DSSC_AEEEM_NPM.csv')\n",
        "results['nasa'] = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/DSSC_NASA_NPM.csv')\n",
        "\n",
        "f1 = {'KNE': 0,\n",
        "      'KNU': 0,\n",
        "      'METADES': 0,\n",
        "      'KNOP': 0,\n",
        "      'OLA': 0,\n",
        "      'LCA': 0,\n",
        "      'RANK': 0,\n",
        "      'MCB': 0}\n",
        "\n",
        "auc = {'KNE': 0,\n",
        "      'KNU': 0,\n",
        "      'METADES': 0,\n",
        "      'KNOP': 0,\n",
        "      'OLA': 0,\n",
        "      'LCA': 0,\n",
        "      'RANK': 0,\n",
        "      'MCB': 0}\n",
        "\n",
        "pf = {'KNE': 0,\n",
        "      'KNU': 0,\n",
        "      'METADES': 0,\n",
        "      'KNOP': 0,\n",
        "      'OLA': 0,\n",
        "      'LCA': 0,\n",
        "      'RANK': 0,\n",
        "      'MCB': 0}      \n",
        "\n",
        "dict_ds = {'F1': f1,\n",
        "            'AUC': auc,\n",
        "            'PF': pf}\n",
        "\n",
        "all_dicts = []\n",
        "metrics_count = dict()\n",
        "# f1, auc, pf = [], [], []\n",
        "for project in results:\n",
        "  ds = results[project]\n",
        "  metrics = ['F1', 'AUC', 'PF']\n",
        "\n",
        "  for m in metrics:\n",
        "    for p in list(np.unique(ds['Project'])):\n",
        "      p_data = ds.loc[ds['Project'] == p]\n",
        "      m_data = p_data[['DS', m]]\n",
        "      print(m_data)\n",
        "      ds_methods = list(np.unique(m_data['DS']))\n",
        "\n",
        "      if m == 'PF':\n",
        "        min = np.min(m_data[m])\n",
        "        best_ds = m_data.loc[m_data[m] == min].drop(m, axis=1).reset_index(drop=True)\n",
        "        best_ds = list(best_ds['DS'])\n",
        "\n",
        "        if len(best_ds) > 1:\n",
        "          for x in best_ds:\n",
        "            pf[x] = pf[x] + 0.5\n",
        "        else:\n",
        "          pf[best_ds[0]] = pf[best_ds[0]] + 1\n",
        "\n",
        "      elif m == 'F1':\n",
        "        max = np.max(m_data[m])\n",
        "        best_ds = m_data.loc[m_data[m] == max].drop(m, axis=1).reset_index(drop=True)\n",
        "        best_ds = list(best_ds['DS'])\n",
        "\n",
        "        if len(best_ds) > 1:\n",
        "          for x in best_ds:\n",
        "            f1[x] = f1[x] + 0.5\n",
        "        else:\n",
        "          f1[best_ds[0]] = f1[best_ds[0]] + 1\n",
        "      \n",
        "    \n",
        "      else:\n",
        "        max = np.max(m_data[m])\n",
        "        best_ds = m_data.loc[m_data[m] == max].drop(m, axis=1).reset_index(drop=True)\n",
        "        best_ds = list(best_ds['DS'])\n",
        "\n",
        "        if len(best_ds) > 1:\n",
        "          for x in best_ds:\n",
        "            auc[x] = auc[x] + 0.5\n",
        "        else:\n",
        "          auc[best_ds[0]] = auc[best_ds[0]] + 1\n",
        "\n",
        "dict_ds['F1'] = f1\n",
        "dict_ds['AUC'] = auc\n",
        "dict_ds['PF'] = pf\n",
        "\n",
        "# print('F1: ', dict_ds['F1'])  \n",
        "# print('AUC: ', dict_ds['AUC'])  \n",
        "# print('PF: ', dict_ds['PF'])\n",
        "\n",
        "res = []\n",
        "for m in dict_ds:\n",
        "  res.append(pd.DataFrame([dict_ds[m].values()], columns=dict_ds[m].keys()))\n",
        "\n",
        "res = pd.concat(res).transpose()\n",
        "res.columns = ['F1-score', 'AUC', 'False Alarm']\n",
        "res['Total'] = res[list(res.columns)].sum(axis=1)\n",
        "res = res.sort_values(by=['Total'], ascending=False)\n",
        "res.to_csv('/content/drive/MyDrive/Colab Notebooks/DSSC/Resultados/Win_Tie.csv', index=False)\n",
        "res\n",
        "  \n",
        "# metrics_count['F1'] = f1\n",
        "# metrics_count['AUC'] = auc\n",
        "# metrics_count['PF'] = pf\n",
        "\n",
        "# metrics_count['F1'] = dict(Counter(metrics_count['F1']))\n",
        "# metrics_count['AUC']  = dict(Counter(metrics_count['AUC']))\n",
        "# metrics_count['PF'] = dict(Counter(metrics_count['PF']))\n",
        "\n",
        "# all_df = []\n",
        "# for i in metrics_count:\n",
        "#   print(i)\n",
        "#   d = metrics_count[i]\n",
        "#   # print(d)\n",
        "#   print(pd.DataFrame(list(d.items()), columns = ['Method', i]))\n",
        "  # all_df.append(pd.DataFrame(list(d.items()), columns = ['Method', i]))\n",
        "  # print(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# result = pd.concat([all_df[0], all_df[1], all_df[2]], axis=1)  \n",
        "# result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-CZjzQxVLn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o82ihQ8mlWA"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "filterwarnings('ignore')\n",
        "\n",
        "res = []\n",
        "for target in list(np.unique(dataset['name'])):\n",
        "  test = dataset.loc[dataset['name'] == target].select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "  train = dataset.loc[dataset['name'] != target].select_dtypes(exclude=['object']).reset_index(drop=True)\n",
        "\n",
        "  X, y = train.drop(train.columns[0], axis=1), train[train.columns[0]]\n",
        "  # X_dsel, y_dsel = X, y\n",
        "  X_test, y_test = test.drop(test.columns[0], axis=1), test[test.columns[0]]\n",
        "  \n",
        "\n",
        "  sampling = [None, RandomOverSampler(), SMOTE()]\n",
        "  print(target)\n",
        "  for DS in list([KNORAU(), METADES(), LCA()]):\n",
        "    for clf in list([LogisticRegression(),\n",
        "                     RandomForestClassifier(),\n",
        "                     GaussianNB(),\n",
        "                     DecisionTreeClassifier()]):\n",
        "      \n",
        "      for sample in sampling:\n",
        "        for scaler in list([None, MinMaxScaler(), StandardScaler()]):\n",
        "          if scaler != None:\n",
        "            X = scaler.fit_transform(X)\n",
        "\n",
        "          if sample != None:\n",
        "            X, y = sample.fit_resample(X, y)\n",
        "\n",
        "          # X_train, X_dsel, y_train, y_dsel = train_test_split(X, y, test_size=0.2)\n",
        "          X_train, y_train = X, y\n",
        "          X_dsel, y_dsel = X, y\n",
        "\n",
        "          pool = BaggingClassifier(base_estimator=clf)\n",
        "          pool.fit(X_train, y_train)\n",
        "          model = DS\n",
        "          model.set_params(pool_classifiers = pool)\n",
        "          model.fit(X_dsel, y_dsel)\n",
        "          if scaler != None:\n",
        "            X_test = scaler.transform(X_test)\n",
        "          y_pred = model.predict(X_test)\n",
        "          y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "          F1 = round(f1_score(y_test, y_pred), 5)\n",
        "          AUC = round(roc_auc_score(y_test, y_prob), 5)\n",
        "          tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "          PF = round(fp / (fp + tn), 5)\n",
        "          string_ds = str(type(DS))\n",
        "          string_ds = string_ds.split(\"'\")[1]\n",
        "          string_ds = string_ds.split(\".\")\n",
        "\n",
        "          name_ds = string_ds[len(string_ds)-1]\n",
        "          values = [target, name_ds, F1, AUC, PF, clf, sample, scaler]\n",
        "          cols = ['Target', 'DS', 'f1', 'auc', 'pf', 'clf', 'sampling', 'scaler']\n",
        "          res.append(pd.DataFrame([values], columns=cols))\n",
        "res = pd.concat(res).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIMDZSpGV2_l"
      },
      "source": [
        "import locale\n",
        "print(locale.format(\"%d\", 1255000, grouping=True))\n",
        "\n",
        "datasets = ['AEEEM', 'NASA', 'PROMISE', 'RELINK']\n",
        "for data_name in datasets:\n",
        "  path = '/content/ml/Datasets/{}/*'.format(data_name)\n",
        "  data = []\n",
        "  for csv in glob(path):\n",
        "    name_csv = csv.split('/')[5]\n",
        "    name_csv = name_csv.split('.csv')[0]\n",
        "    ds = pd.read_csv(csv)\n",
        "    bugs = list(ds[ds.columns[0]].value_counts())[1]\n",
        "    instances = len(ds)\n",
        "    percent = round((bugs / instances)*100, 2)\n",
        "    effort = np.sum(ds[ds.columns[1]])\n",
        "    loc_metric = list(ds.columns)[1]\n",
        "    print(loc_metric)\n",
        "    values = [data_name, name_csv, instances, bugs, percent, effort, loc_metric]\n",
        "    cols = ['Dataset', 'Project', 'Instances', 'Deffects', '%', 'Effort', 'Effort Metric']\n",
        "    data.append(pd.DataFrame([values], columns=cols))\n",
        "  data = pd.concat(data).reset_index(drop=True)\n",
        "  data = data.sort_values(by='Project', ascending=True)\n",
        "  data.loc['Column_Total']= data.sum(numeric_only=True, axis=0)\n",
        "  data.to_csv('/content/sample_data/{}_Instance_Effort.csv'.format(data_name), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocRBrMYp7BNH"
      },
      "source": [
        "df = pd.read_csv('/content/ml/Datasets/RELINK/Apache.csv')\n",
        "df = df.sort_values(by='CountLineCodeExe', ascending=True).reset_index(drop=True)\n",
        "effort =  (20 * df['CountLineCodeExe'].sum())/100\n",
        "index = df['CountLineCodeExe'].cumsum().searchsorted(effort)\n",
        "TargetList = df[:index]\n",
        "\n",
        "LOC = 'CountLineCodeExe'\n",
        "effort_percent = np.arange(0, 101, 1)\n",
        "defective_list = []\n",
        "\n",
        "for percent in range(0, 101):\n",
        "  data = []\n",
        "  effort_loc =  (percent * target_list['CountLineCodeExe'].sum())/100\n",
        "  index = target_list['CountLineCodeExe'].cumsum().searchsorted(effort_loc)\n",
        "  data = df[:index]\n",
        "\n",
        "  if len(data) != 0:\n",
        "    \n",
        "    bugs = np.count_nonzero(data[data.columns[0]] == 1)\n",
        "    percent_bugs = bugs / np.count_nonzero(TargetList[TargetList.columns[0]] == 1)\n",
        "    defective_list.append(percent_bugs)\n",
        "  else:\n",
        "    defective_list.append(0.0)\n",
        "\n",
        "\n",
        "x = effort_percent\n",
        "y = defective_list\n",
        "\n",
        "y_a = np.arange(0.0, 1.01, 0.02)\n",
        "x_b = np.arange(0, 101, 2)\n",
        "\n",
        "x_a, y_b, h = [], [], []\n",
        "for i in range(51):\n",
        "  x_a.append(0)\n",
        "  y_b.append(1)\n",
        "  h.append(100)\n",
        "\n",
        "x1 = np.concatenate([x_a, x_b])\n",
        "y1  = np.concatenate([y_a, y_b])\n",
        "\n",
        "x2 = np.concatenate([x_b, h])\n",
        "y2 = np.concatenate([x_a, y_a])\n",
        "\n",
        "area_P_R = auc(x, y) - auc(np.arange(0, 100, 1), np.arange(0.00, 1.0, 0.01))\n",
        "area_O_P = auc(x1, y1) - auc(x, y)\n",
        "area_P_R = auc(x, y) - auc(np.arange(0, 101, 1), np.arange(0.00, 1.01, 0.01))\n",
        "area_R_W = auc(np.arange(0, 100, 1), np.arange(0.00, 1.0, 0.01)) - auc(x2, y2)\n",
        "\n",
        "popt = 1 - (area_O_P/ (area_O_P + area_P_R + area_R_W))\n",
        "\n",
        "\n",
        "print(popt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "AOWrfpKuj455",
        "outputId": "9889ad66-2ffe-492d-cad7-c0b363cb3a85"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isDefective</th>\n",
              "      <th>CountLineCodeExe</th>\n",
              "      <th>AvgCyclomatic</th>\n",
              "      <th>AvgCyclomaticModified</th>\n",
              "      <th>AvgCyclomaticStrict</th>\n",
              "      <th>AvgEssential</th>\n",
              "      <th>AvgLine</th>\n",
              "      <th>AvgLineBlank</th>\n",
              "      <th>AvgLineCode</th>\n",
              "      <th>AvgLineComment</th>\n",
              "      <th>CountLine</th>\n",
              "      <th>CountLineBlank</th>\n",
              "      <th>CountLineCode</th>\n",
              "      <th>CountLineCodeDecl</th>\n",
              "      <th>CountLineComment</th>\n",
              "      <th>CountSemicolon</th>\n",
              "      <th>CountStmt</th>\n",
              "      <th>CountStmtDecl</th>\n",
              "      <th>CountStmtExe</th>\n",
              "      <th>MaxCyclomatic</th>\n",
              "      <th>MaxCyclomaticModified</th>\n",
              "      <th>MaxCyclomaticStrict</th>\n",
              "      <th>RatioCommentToCode</th>\n",
              "      <th>SumCyclomatic</th>\n",
              "      <th>SumCyclomaticModified</th>\n",
              "      <th>SumCyclomaticStrict</th>\n",
              "      <th>SumEssential</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>907</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>61</td>\n",
              "      <td>7</td>\n",
              "      <td>2195</td>\n",
              "      <td>165</td>\n",
              "      <td>1185</td>\n",
              "      <td>191</td>\n",
              "      <td>328</td>\n",
              "      <td>719</td>\n",
              "      <td>916</td>\n",
              "      <td>178</td>\n",
              "      <td>738</td>\n",
              "      <td>51</td>\n",
              "      <td>51</td>\n",
              "      <td>56</td>\n",
              "      <td>0.28</td>\n",
              "      <td>221</td>\n",
              "      <td>197</td>\n",
              "      <td>246</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>644</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1096</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>4</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>1945</td>\n",
              "      <td>209</td>\n",
              "      <td>1607</td>\n",
              "      <td>185</td>\n",
              "      <td>65</td>\n",
              "      <td>19001</td>\n",
              "      <td>1049</td>\n",
              "      <td>172</td>\n",
              "      <td>877</td>\n",
              "      <td>45</td>\n",
              "      <td>26</td>\n",
              "      <td>45</td>\n",
              "      <td>0.04</td>\n",
              "      <td>256</td>\n",
              "      <td>217</td>\n",
              "      <td>277</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>522</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>5</td>\n",
              "      <td>1159</td>\n",
              "      <td>171</td>\n",
              "      <td>727</td>\n",
              "      <td>137</td>\n",
              "      <td>236</td>\n",
              "      <td>397</td>\n",
              "      <td>526</td>\n",
              "      <td>112</td>\n",
              "      <td>414</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>30</td>\n",
              "      <td>0.32</td>\n",
              "      <td>134</td>\n",
              "      <td>122</td>\n",
              "      <td>146</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>166</td>\n",
              "      <td>15</td>\n",
              "      <td>55</td>\n",
              "      <td>13</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.73</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>354</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>7</td>\n",
              "      <td>1072</td>\n",
              "      <td>126</td>\n",
              "      <td>607</td>\n",
              "      <td>119</td>\n",
              "      <td>280</td>\n",
              "      <td>309</td>\n",
              "      <td>428</td>\n",
              "      <td>107</td>\n",
              "      <td>321</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>0.46</td>\n",
              "      <td>122</td>\n",
              "      <td>122</td>\n",
              "      <td>148</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>697</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0</td>\n",
              "      <td>205</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>100</td>\n",
              "      <td>7</td>\n",
              "      <td>83</td>\n",
              "      <td>3</td>\n",
              "      <td>375</td>\n",
              "      <td>34</td>\n",
              "      <td>268</td>\n",
              "      <td>49</td>\n",
              "      <td>44</td>\n",
              "      <td>144</td>\n",
              "      <td>196</td>\n",
              "      <td>35</td>\n",
              "      <td>161</td>\n",
              "      <td>46</td>\n",
              "      <td>33</td>\n",
              "      <td>56</td>\n",
              "      <td>0.16</td>\n",
              "      <td>51</td>\n",
              "      <td>38</td>\n",
              "      <td>62</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>1</td>\n",
              "      <td>866</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2136</td>\n",
              "      <td>224</td>\n",
              "      <td>1322</td>\n",
              "      <td>289</td>\n",
              "      <td>474</td>\n",
              "      <td>666</td>\n",
              "      <td>962</td>\n",
              "      <td>341</td>\n",
              "      <td>621</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "      <td>0.36</td>\n",
              "      <td>205</td>\n",
              "      <td>191</td>\n",
              "      <td>229</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0</td>\n",
              "      <td>244</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>567</td>\n",
              "      <td>95</td>\n",
              "      <td>355</td>\n",
              "      <td>77</td>\n",
              "      <td>129</td>\n",
              "      <td>252</td>\n",
              "      <td>313</td>\n",
              "      <td>73</td>\n",
              "      <td>240</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>25</td>\n",
              "      <td>0.36</td>\n",
              "      <td>66</td>\n",
              "      <td>61</td>\n",
              "      <td>74</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>194 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     isDefective  CountLineCodeExe  ...  SumCyclomaticStrict  SumEssential\n",
              "0              1               907  ...                  246            81\n",
              "1              0                 0  ...                    0             0\n",
              "2              1              1096  ...                  277           128\n",
              "3              0               522  ...                  146            63\n",
              "4              0                26  ...                    9            10\n",
              "..           ...               ...  ...                  ...           ...\n",
              "189            1               354  ...                  148            79\n",
              "190            0                 0  ...                    0             0\n",
              "191            0               205  ...                   62            25\n",
              "192            1               866  ...                  229            89\n",
              "193            0               244  ...                   74            21\n",
              "\n",
              "[194 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az0qVUn0DwHY"
      },
      "source": [
        ""
      ]
    }
  ]
}